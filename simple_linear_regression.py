import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
sns.set()


''' Data Generating Process (DGP) '''
np.random.seed(100)
# Here we define the data generating process (DGP):
N = 1000                                                    # The number of observations
σ = 500                                                     # The standard deviation of the disturbances (errors)
ε = np.random.randn(N) * σ                                  # The errors will be normally distributed with N(0, σ^2)
α = 100                                                     # The intercept
β = 1.75                                                    # The slope
x = np.linspace(start=0, stop=2000, num=N, endpoint=True)   # The x-values
y = α + β * x + ε                                           # The y-values, generated by a linear model

# We will subdivide the results of DGP into an in-sample (what we might observe in reality) and a testing set
idx_train = np.random.choice(np.arange(len(x)), int(0.85 * N), replace=False)
x_train = x[idx_train]
y_train = y[idx_train]
y_test = y[~idx_train]
x_test = x[~idx_train]

# Here we plot the in-sample data, that we will use to predict the "True" model (red line)
plt.scatter(x_train, y_train, label='Scatter-plot of training values')
plt.plot(x_train, α + β * x_train, color='r', label='The DGP true model')
plt.legend()
plt.title('Training Data')
plt.show()


''' Least Squares '''
# Now we will use least squares method to estimate the red line in the previous plot
# We want to estimate α and β by "a" and "b" by minimising the sum of the squared error term "e^2"
# where e = y - a - bx
# Our goal is then to minimise Σe_i^2
y_train_mean = np.mean(y_train)
x_train_mean = np.mean(x_train)
b = np.sum((x_train - x_train_mean) * (y_train - y_train_mean)) / np.sum((x_train - x_train_mean) ** 2)
a = y_train_mean - b * x_train_mean
e = y_train - a - b * x_train
fig, ax = plt.subplots(2, 1)
ax[0].scatter(x_train, y_train, label='Scatter-plot of training values')
ax[0].plot(x_train, α + β * x_train, color='r', label='The DGP true model')
ax[0].plot(x_train, a + b * x_train, color='g', label='LS regression estimated model')
ax[0].legend()
ax[0].set_title('Training Data')
ax[1].hist(e, bins=50)
ax[1].set_title('Error Distribution')
plt.show()


''' Residuals and R^2'''

# The least squares residuals are given by e_i = y_i - a - b * x_i
# Given the way we have constructed the model, we find Σe_i = 0 and Σe_i*(x_i - x_bar) = 0
# Traditional way to assess the performance of least squares is to compare the sum of squared resioduals with the sum
# of squares to (y_i - y_bar)
# Σ(y_i - y_bar)^2 = b^2 * Σ(x_i - x_bar)^2 + Σe_i^2
#     SST          =       SSE              +  SSR
# R^2 us given by SSE / SST
SST_train = np.sum((y_train - y_train_mean) ** 2)
SSE_train = (b ** 2) * np.sum((x_train - x_train_mean) ** 2)
R_2_train = SSE_train / SST_train
print(R_2_train)


''' Accuracy of Least Squares '''
# As we (in this setup) know the true model parameters, α and β, we can compare our estimates "a" and "b" to these.
M = 100
a_sim = np.zeros(shape=(M,))
b_sim = np.zeros(shape=(M,))
e_sim = np.zeros(shape=(M, N))
for m in range(100):
    # Repeat the DGP M times
    np.random.seed(m)
    ε = np.random.randn(N) * σ
    x = np.linspace(start=0, stop=2000, num=N, endpoint=True)
    y = α + β * x + ε
    y_mean = np.mean(y)
    x_mean = np.mean(x)
    b_sim[m] = np.sum((x - x_mean) * (y - y_mean)) / np.sum((x - x_mean) ** 2)
    a_sim[m] = y_mean - b_sim[m] * x_mean
    e_sim[m] = y - a_sim[m] - b_sim[m] * x

a_bar = np.mean(a_sim)
b_bar = np.mean(b_sim)
MSE_a = np.mean((a - α) ** 2)
MSE_b = np.mean((b - β) ** 2)

print(f'Summary of the M runs is:\n'
      f'a_bar: {a_bar:.2f}, (True value α: {α})\n'
      f'b_bar: {b_bar:.2f}, (True value α: {β})\n'
      f'MSE_a: {MSE_a:.2f},\n'
      f'MSE_b: {MSE_b:.2f}.')

''' Seven Assumptions '''
# 1 Fixed regressors (the x_is are fixed and have non-zero variance)
# 2 Random disturbances, zero mean. ε are random and have E[ε_i] = 0
# 3 Homoskedasticity. The variance of the disturbances are all equal E[ε_i^2] = σ for all i
# 4 No Correlation. The ε are uncorrelated E[ε_iε_j] = 0 (for all i, j = 1, ... n, i != j)
# 5 Constant parameters. α, β, and σ are fixed unknown numbers with σ > 0.
# 6 Linear model. y_i =  α, + βx_i + ε_i
# 7 Normality. The disturbances are normally distributed (not always necessary but here we assumed it).


''' Significance Test'''
s = np.sqrt(1 / (int(0.85 * N) - 2) * np.sum(e ** 2))
s_b = s / np.sqrt(np.sum((x_train - x_train_mean) ** 2))
t_b = b / s_b
t_dist = np.random.standard_t(int(0.85 * N) - 2, size=100000)

plt.hist(t_dist, bins=100, color='blue')
plt.axvline(t_b, color='r')
plt.axvline(1.96, color='black')
plt.axvline(-1.96, color='black')
plt.show()
# Given the above plot, we might safely reject the null hypothesis and accept the alternative
# that b is non-zero


''' Prediction '''
# For a given x_i we can predict the corresponding y_i under our model (y_i = a + b * x_i)
# How good is this prediction?

# We will use our testing set to check properties of our prediction

f = y_test - a - b * x_test
s_2 = 1 / (int(0.85 * N) - 2) * np.sum(e ** 2)
s_f_2 = s_2 * (1 + 1 / int(0.85 * N) + (x_test - x_train_mean) ** 2 / np.sum((x_train - x_train_mean) ** 2))
s_f = np.sqrt(s_f_2)
c975 = 1.96
c90 = 1.28
plt.scatter(x_test, y_test, color='blue', label='True Responses')
plt.plot(x_test, a + b * x_test, color='red', label='Predicted Responses')
plt.plot(x_test, a + b * x_test - c975 * s_f, color='green', ls='--', label='Lower 97.5% Confidence Bar')
plt.plot(x_test, a + b * x_test + c975 * s_f, color='green', ls='--', label='Upper 97.5% Confidence Bar')
plt.plot(x_test, a + b * x_test - c90 * s_f, color='black', ls='--', label='Lower 90% Confidence Bar')
plt.plot(x_test, a + b * x_test + c90 * s_f, color='black', ls='--', label='Upper 90% Confidence Bar')
plt.legend()
plt.title('Predictions with Confidence Intervals')
plt.show()



